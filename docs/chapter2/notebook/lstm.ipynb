{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置\n",
    "--------------------------------------------------------\n",
    "```python\n",
    "pip -m pip install --upgrade pip\n",
    "# 更换 pypi 源加速库的安装\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "pip install torch==2.5.1\n",
    "pip install torchvision==0.20.1\n",
    "pip install swanlab==0.3.23\n",
    "pip install scikit-learn==1.5.2\n",
    "pip install pandas==2.0.3\n",
    "pip install matplotlib==3.7.2\n",
    "```\n",
    "--------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者你也可以使用 `conda` 来管理你的环境\n",
    "------------------------------------------------------------\n",
    "``` python\n",
    "conda create -n lstm python==3.10\n",
    "\n",
    "conda activate lstm\n",
    "\n",
    "pip install uv && uv pip install -r requirements.txt\n",
    "```\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包的引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数的定义，这里显式的给出，在之后的代码里我们将使用 swanlab 来管理超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30\n",
    "learning_rate = 0.005\n",
    "hidden_units = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些网络中会用到的激活函数，这里给出主要是为了展示其定义和数学表达，实际中不论是 numpy 库，还是 pytorch 库，都已经帮我们实现好了，我们直接调用即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:4: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/xy/2nl06h6134z8d63822qpjs840000gn/T/ipykernel_48493/1941595322.py:4: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  return 1/1(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/1(1+np.exp(-X))\n",
    "\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "# softmax activation\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X, axis=1).reshape(-1, 1)\n",
    "    exp_X = exp_X / exp_X_sum\n",
    "    return exp_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/rnn_vs_lstm.png)\n",
    "这一部分进行LSTM 网络状态的初始化，由上图以及前面学习的知识可知，LSTM 网络的状态有两个，一个是 $C_t$，一个是 $H_t$，我们需要分别对这两个状态进行初始化，并封装成一个函数`init_lstm_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 lstm，包含cell state, hidden state\n",
    "def init_lstm_state(batch_size, hidden_units, device):\n",
    "    return (torch.zeros((batch_size, hidden_units), device=device), \n",
    "            torch.zeros((batch_size, hidden_units), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面一部分进行各参数的初始化，首先定义一个`normal`函数用于生成满足正态分布的Tensor形式的数据。\n",
    "\n",
    "后面再定义包含遗忘门，输入门，输出门，候选记忆单元，隐藏层/输出层的参数。并使用一个参数字典统一管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def initialize_parameters(vocab_size, hidden_units, device):\n",
    "    std = 0.01\n",
    "    input_units = output_units = vocab_size\n",
    "\n",
    "    # 正态分布\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * std\n",
    "\n",
    "    # LSTM cell weights\n",
    "    forget_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    input_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    output_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    c_tilda_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "\n",
    "    # 偏置项\n",
    "    forget_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    input_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    output_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    c_tilda_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "\n",
    "    # 输出层参数\n",
    "    hidden_output_weights = normal((hidden_units, output_units))\n",
    "    output_bias = torch.zeros((1, output_units), device=device)\n",
    "\n",
    "    # 将所有参数添加到字典\n",
    "    parameters = {\n",
    "        'fgw': forget_gate_weights,\n",
    "        'igw': input_gate_weights,\n",
    "        'ogw': output_gate_weights,\n",
    "        'cgw': c_tilda_gate_weights,\n",
    "        'fgb': forget_gate_bias,\n",
    "        'igb': input_gate_bias,\n",
    "        'ogb': output_gate_bias,\n",
    "        'cgb': c_tilda_gate_bias,\n",
    "        'how': hidden_output_weights,\n",
    "        'ob': output_bias\n",
    "    }\n",
    "\n",
    "    # 设置 requires_grad=True 以启用梯度计算\n",
    "    # 确保所有参数在反向传播中能够计算梯度\n",
    "    for param in parameters.values():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这一块就开始 lstm 的代码实现了，阅读这部分时确保你已经对相关的公式有所了解。\n",
    "\n",
    "在代码内部，我们首先从参数字典中读取出相关的参数，然后将传入的当前批次数据与历史的 hidden state 进行串联。接着再依次通过每个\"门\"，并且计算LSTM 的 cell_state 与 hidden state。\n",
    "\n",
    "有的同学可能就会有疑问了，代码中实现的公式和理论讲解中的公式不太一样呀，以遗忘门为例：\n",
    "\n",
    "理论讲解中的公式是： \n",
    "$F_t = \\sigma(X_t W_{xf} + H_{t-1} W_{hf} + b_f)$\n",
    "\n",
    "对于遗忘门结构应该有两个权重参数矩阵，$W_{xf}$ 与 $W_{hf}$，而代码中只有一个参数矩阵 `fgw`，并且理论公式中，我们将 $X_t$ 与 $H_{t-1}$ 与权重矩阵相乘后才进行的拼接，而代码中却直接拼接了，这是为什么呢？我们来一起分析一下这个问题\n",
    "\n",
    "以股票预测问题为例，我们要用过去 7 天的股票数据来预测第 8 天的股票价格，那么我们的输入数据 $X_t$ 就是一个 `batch_size * 7` 的向量，其中 `batch_size` 表示当前批次的样本数量，7 表示过去 7 天的股票价格。\n",
    "\n",
    "那么假设 $X_t$ 是一个 $batch_size$ * 7 的向量 ， $H_{t-1}$是一个 `batch_size * hidden_units` 的向量，那么 $X_t$ 与 $H_{t-1}$ 通过 `torch.cat` 或者 `np.concatenate` 拼接后得到的新向量是 $batch_size * (7 + hidden_units)$。\n",
    "\n",
    "对于权重矩阵，相应的，我们在上一个`code cell`定义其形状为`(input_units + hidden_units, hidden_units)`，其中 `input_units` 实际上就是 7，可以看出上这里定义的权重矩阵包含两个部分，$W_{xf} 与 W_{hf}$，前者形状为 `input_units * hidden_units`，后者形状为 `hidden_units * hidden_units`\n",
    "\n",
    "这样，原始的两个独立的矩阵乘法和加法运算$X_t W_{xf} + H_{t-1} W_{hf}$可以被单独重写成一个矩阵乘法，即 $X_t W_{xf} + H_{t-1} W_{hf} = concat\\_ dataset  W$。这种重写不仅简化了表达式，还使得实现更加高效，因为可以少维护了一个权重矩阵，在计算上更简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 加公式\n",
    "\n",
    "# single lstm cell\n",
    "def lstm_cell(batch_dataset, prev_hidden_state, prev_cell_state, parameters):\n",
    "    # get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    cgw = parameters['cgw']\n",
    "\n",
    "    fgb = parameters['fgb']\n",
    "    igb = parameters['igb']\n",
    "    ogb = parameters['ogb']\n",
    "    cgb = parameters['cgb']\n",
    "    \n",
    "    # 串联 data 和 prev_hidden_state  \n",
    "    # concat_dataset = torch.cat((batch_dataset, prev_hidden_state), dim=1)\n",
    "    concat_dataset = np.concatenate((batch_dataset, prev_hidden_state), axis=1)\n",
    "\n",
    "    # forget gate activations\n",
    "    F = sigmoid(np.matmul(concat_dataset, fgw) + fgb)\n",
    "\n",
    "    # input gate activations\n",
    "    I = sigmoid(np.matmul(concat_dataset, igw) + igb)\n",
    "\n",
    "    # output gate activations\n",
    "    O = sigmoid(np.matmul(concat_dataset, ogw) + ogb)\n",
    "\n",
    "    # cell_tilda gate activations\n",
    "    C_tilda = np.tanh(np.matmul(concat_dataset, cgw) + cgb)\n",
    "\n",
    "    # 更新 cell state, hidden_state\n",
    "    cell_state = F * prev_cell_state + I * C_tilda\n",
    "    hidden_state = np.multiply(O, np.tanh(cell_state))\n",
    "\n",
    "    # store four gate weights to be used in back propagation\n",
    "    lstm_activations = {\n",
    "        'F': F,\n",
    "        'I': I,\n",
    "        'O': O,\n",
    "        'C_tilda': C_tilda\n",
    "    }\n",
    "    \n",
    "    return lstm_activations, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出层\n",
    "# 需要注意的是，只有隐状态才会传递到输出层，而记忆元不直接参与输出计算，记忆元完全属于内部信息\n",
    "def output_cell(hidden_state, parameters):\n",
    "    # get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    ob = parameters['ob']\n",
    "    # calculate the output\n",
    "    output = np.matmul(hidden_state, how) + ob\n",
    "    # 如果输出为概率的话，可以使用softmax函数进行归一化\n",
    "    # output = softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义完 `lstm_cell` 以及 `output_cell` 之后，我们在其之上定义了一个 lstm ，负责输入数据并拿到输出，`lstm`函数包含三个参数，`batch_dataset` 表示一批输入数据，`initial_state` 表示初始化状态的一个函数，parameters 表示当前模型的参数。\n",
    "\n",
    "我们先初始化模型 state，然后依次通过 `lstm_cell` 拿到每个时间步的输出，最后通过 `output_cell` 拿到最终的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(batch_dataset, initail_state, parameters):\n",
    "    hidden_state, cell_state = initail_state\n",
    "    outputs = []\n",
    "    _, hidden_state, cell_state = lstm_cell(batch_dataset, hidden_state, cell_state, parameters)\n",
    "    outputs.append(output_cell(hidden_state, parameters))    \n",
    "    return outputs, (hidden_state, cell_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做完上述工作，我们已经解决了前向传播的问题，还需要封装一个简单的 RNN类来做模型参数的初始化，状态的初始化以及前向传播。\n",
    "\n",
    "`__call__`方法的作用是使实例对象可以像调用普通函数那样，以“对象名()”的形式使用。完成 lstm 的前向传播 `forward_fn`，我们只需要将其指定为上述所定义的`lstm`函数即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个RNN 类来训练LSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModelScratch:\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.params = get_params(vocab_size, hidden_units, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        # 根据任务不同灵活对输入数据进行预先处理\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        # X = X.type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RNNModelScratch at 0x155083130>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModelScratch(vocab_size, hidden_units, device, initialize_parameters, init_lstm_state, lstm)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**展示模型的参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fgw': tensor([[-0.0129,  0.0029, -0.0024,  ..., -0.0031,  0.0026, -0.0097],\n",
       "         [-0.0105, -0.0057, -0.0095,  ...,  0.0121,  0.0177, -0.0036],\n",
       "         [ 0.0069,  0.0037, -0.0084,  ...,  0.0045,  0.0112,  0.0222],\n",
       "         ...,\n",
       "         [ 0.0178, -0.0045,  0.0048,  ..., -0.0028, -0.0112, -0.0143],\n",
       "         [-0.0027,  0.0034,  0.0107,  ..., -0.0064,  0.0085,  0.0130],\n",
       "         [-0.0070,  0.0030, -0.0072,  ...,  0.0096, -0.0125,  0.0097]],\n",
       "        requires_grad=True),\n",
       " 'igw': tensor([[ 0.0003,  0.0072, -0.0107,  ..., -0.0024, -0.0070, -0.0120],\n",
       "         [ 0.0151,  0.0166, -0.0284,  ..., -0.0058, -0.0040, -0.0056],\n",
       "         [-0.0079, -0.0048, -0.0048,  ..., -0.0017, -0.0081, -0.0022],\n",
       "         ...,\n",
       "         [-0.0239,  0.0221,  0.0033,  ...,  0.0129, -0.0150, -0.0046],\n",
       "         [-0.0017, -0.0003,  0.0045,  ..., -0.0064,  0.0023,  0.0043],\n",
       "         [-0.0104,  0.0113,  0.0076,  ...,  0.0147,  0.0108, -0.0085]],\n",
       "        requires_grad=True),\n",
       " 'ogw': tensor([[ 0.0052,  0.0051,  0.0007,  ..., -0.0007, -0.0156, -0.0086],\n",
       "         [-0.0079, -0.0078, -0.0069,  ...,  0.0171,  0.0044, -0.0043],\n",
       "         [ 0.0055,  0.0207, -0.0039,  ...,  0.0017, -0.0040, -0.0188],\n",
       "         ...,\n",
       "         [-0.0232,  0.0135,  0.0068,  ..., -0.0017, -0.0032,  0.0037],\n",
       "         [ 0.0122, -0.0045, -0.0130,  ...,  0.0177,  0.0129,  0.0033],\n",
       "         [ 0.0016,  0.0036, -0.0026,  ..., -0.0029,  0.0040,  0.0013]],\n",
       "        requires_grad=True),\n",
       " 'cgw': tensor([[ 0.0138,  0.0099, -0.0125,  ...,  0.0078, -0.0059,  0.0108],\n",
       "         [-0.0117, -0.0180, -0.0131,  ..., -0.0056, -0.0296,  0.0055],\n",
       "         [ 0.0009,  0.0170, -0.0059,  ...,  0.0026,  0.0200, -0.0031],\n",
       "         ...,\n",
       "         [ 0.0083, -0.0007, -0.0012,  ...,  0.0063,  0.0033,  0.0181],\n",
       "         [-0.0071, -0.0068,  0.0106,  ...,  0.0032, -0.0072,  0.0089],\n",
       "         [-0.0085,  0.0151, -0.0151,  ...,  0.0018,  0.0084,  0.0040]],\n",
       "        requires_grad=True),\n",
       " 'fgb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'igb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'ogb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'cgb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'how': tensor([[-0.0070,  0.0021,  0.0006,  ..., -0.0049,  0.0111, -0.0052],\n",
       "         [-0.0008, -0.0125, -0.0029,  ..., -0.0038, -0.0053, -0.0038],\n",
       "         [-0.0210, -0.0028,  0.0008,  ..., -0.0080, -0.0012,  0.0115],\n",
       "         ...,\n",
       "         [-0.0011,  0.0050,  0.0214,  ...,  0.0040,  0.0125,  0.0001],\n",
       "         [-0.0101, -0.0028, -0.0076,  ..., -0.0067, -0.0178, -0.0107],\n",
       "         [ 0.0133,  0.0018,  0.0018,  ..., -0.0031, -0.0073, -0.0072]],\n",
       "        requires_grad=True),\n",
       " 'ob': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]], requires_grad=True)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**恭喜你🎉**\n",
    "\n",
    "至此你已经能完成 `lstm` 模型的前向传播了，`model.py`的内容与本篇代码类似。在下一个阶段，我们将会通过一个股票预测问题，来应用我们的lstm模型，同时加入反向传播与参数更新部分。具体代码在 `main.py`中。运行下列命令就可以开始训练了\n",
    "\n",
    "ps: 因为过程中使用了 `swanlab` 这个工具来做可视化，所以您需要先注册一个账号并在终端中贴入你的 `api_key`\n",
    "\n",
    "swanlab: [swanlab.cn](https://swanlab.cn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3852.87s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<model.RNNModelScratch object at 0x132dcba30>\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.3.23                                  \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/Users/little1d/Desktop/Playground/LSTM-From-Scratch/notebook/swanlog/run-20241108_165219-2a23d349\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mHarrison\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mLSTM\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🌟 Run `\u001b[1mswanlab watch /Users/little1d/Desktop/Playground/LSTM-From-Scratch/notebook/swanlog\u001b[0m` to view SwanLab Experiment Dashboard locally\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction/runs/9ukh7j8s98w9sdw6y7sis\u001b[0m\u001b[0m\n",
      "Epoch 1, Loss: 0.2294756778412395\n",
      "Epoch 2, Loss: 0.011775485281961866\n",
      "Epoch 3, Loss: 0.002961122291000922\n",
      "Epoch 4, Loss: 0.002451772078832922\n",
      "Epoch 5, Loss: 0.0021021694523773882\n",
      "Epoch 6, Loss: 0.0018974299099580902\n",
      "Epoch 7, Loss: 0.0017400443749566977\n",
      "Epoch 8, Loss: 0.0016426904540922907\n",
      "Epoch 9, Loss: 0.0015705324921226646\n",
      "Epoch 10, Loss: 0.0015044413885334507\n",
      "Epoch 11, Loss: 0.0015101283578486699\n",
      "Epoch 12, Loss: 0.0014672611287096515\n",
      "Epoch 13, Loss: 0.0013940357619301518\n",
      "Epoch 14, Loss: 0.0013344537227466288\n",
      "Epoch 15, Loss: 0.0012397624283645807\n",
      "Epoch 16, Loss: 0.0012222831759976947\n",
      "Epoch 17, Loss: 0.00120571398777732\n",
      "Epoch 18, Loss: 0.0011475305089132031\n",
      "Epoch 19, Loss: 0.0011208955499265965\n",
      "Epoch 20, Loss: 0.00112965861788123\n",
      "Epoch 21, Loss: 0.0010640612397562815\n",
      "Epoch 22, Loss: 0.0010619352744672345\n",
      "Epoch 23, Loss: 0.0010453782055669257\n",
      "Epoch 24, Loss: 0.0010723455278720292\n",
      "Epoch 25, Loss: 0.0010933163535406089\n",
      "Epoch 26, Loss: 0.0010416477962280624\n",
      "Epoch 27, Loss: 0.00098799324930749\n",
      "Epoch 28, Loss: 0.0009662236973074161\n",
      "Epoch 29, Loss: 0.0009593831621006959\n",
      "Epoch 30, Loss: 0.0009458516764829659\n",
      "Epoch 31, Loss: 0.0009294575825656971\n",
      "Epoch 32, Loss: 0.0009261835334149913\n",
      "Epoch 33, Loss: 0.0009095736661240355\n",
      "Epoch 34, Loss: 0.000903696504893661\n",
      "Epoch 35, Loss: 0.0008960804625530727\n",
      "Epoch 36, Loss: 0.0008970295360389476\n",
      "Epoch 37, Loss: 0.0008677644830944095\n",
      "Epoch 38, Loss: 0.0008739470116173228\n",
      "Epoch 39, Loss: 0.0008506751659701371\n",
      "Epoch 40, Loss: 0.0008439103516543077\n",
      "Epoch 41, Loss: 0.0008311234907725722\n",
      "Epoch 42, Loss: 0.0008459278085663553\n",
      "Epoch 43, Loss: 0.0008125396517344699\n",
      "Epoch 44, Loss: 0.0008044609138677414\n",
      "Epoch 45, Loss: 0.0008081319105662664\n",
      "Epoch 46, Loss: 0.0007828941525076516\n",
      "Epoch 47, Loss: 0.000788869156773823\n",
      "Epoch 48, Loss: 0.0007756685073319306\n",
      "Epoch 49, Loss: 0.0007679181653656997\n",
      "Epoch 50, Loss: 0.0007690867261974037\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Experiment \u001b[33mLSTM\u001b[0m has completed\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🌟 Run `\u001b[1mswanlab watch /Users/little1d/Desktop/Playground/LSTM-From-Scratch/notebook/swanlog\u001b[0m` to view SwanLab Experiment Dashboard locally\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction/runs/9ukh7j8s98w9sdw6y7sis\u001b[0m\u001b[0m\n",
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
