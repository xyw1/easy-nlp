{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¯å¢ƒé…ç½®\n",
    "--------------------------------------------------------\n",
    "```python\n",
    "pip -m pip install --upgrade pip\n",
    "# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "pip install torch==2.5.1\n",
    "pip install torchvision==0.20.1\n",
    "pip install swanlab==0.3.23\n",
    "pip install scikit-learn==1.5.2\n",
    "pip install pandas==2.0.3\n",
    "pip install matplotlib==3.7.2\n",
    "```\n",
    "--------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ–è€…ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `conda` æ¥ç®¡ç†ä½ çš„ç¯å¢ƒ\n",
    "------------------------------------------------------------\n",
    "``` python\n",
    "conda create -n lstm python==3.10\n",
    "\n",
    "conda activate lstm\n",
    "\n",
    "pip install uv && uv pip install -r requirements.txt\n",
    "```\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒ…çš„å¼•å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¶…å‚æ•°çš„å®šä¹‰ï¼Œè¿™é‡Œæ˜¾å¼çš„ç»™å‡ºï¼Œåœ¨ä¹‹åçš„ä»£ç é‡Œæˆ‘ä»¬å°†ä½¿ç”¨ swanlab æ¥ç®¡ç†è¶…å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30\n",
    "learning_rate = 0.005\n",
    "hidden_units = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€äº›ç½‘ç»œä¸­ä¼šç”¨åˆ°çš„æ¿€æ´»å‡½æ•°ï¼Œè¿™é‡Œç»™å‡ºä¸»è¦æ˜¯ä¸ºäº†å±•ç¤ºå…¶å®šä¹‰å’Œæ•°å­¦è¡¨è¾¾ï¼Œå®é™…ä¸­ä¸è®ºæ˜¯ numpy åº“ï¼Œè¿˜æ˜¯ pytorch åº“ï¼Œéƒ½å·²ç»å¸®æˆ‘ä»¬å®ç°å¥½äº†ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:4: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/xy/2nl06h6134z8d63822qpjs840000gn/T/ipykernel_48493/1941595322.py:4: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  return 1/1(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/1(1+np.exp(-X))\n",
    "\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "# softmax activation\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X, axis=1).reshape(-1, 1)\n",
    "    exp_X = exp_X / exp_X_sum\n",
    "    return exp_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/rnn_vs_lstm.png)\n",
    "è¿™ä¸€éƒ¨åˆ†è¿›è¡ŒLSTM ç½‘ç»œçŠ¶æ€çš„åˆå§‹åŒ–ï¼Œç”±ä¸Šå›¾ä»¥åŠå‰é¢å­¦ä¹ çš„çŸ¥è¯†å¯çŸ¥ï¼ŒLSTM ç½‘ç»œçš„çŠ¶æ€æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯ $C_t$ï¼Œä¸€ä¸ªæ˜¯ $H_t$ï¼Œæˆ‘ä»¬éœ€è¦åˆ†åˆ«å¯¹è¿™ä¸¤ä¸ªçŠ¶æ€è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶å°è£…æˆä¸€ä¸ªå‡½æ•°`init_lstm_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ– lstmï¼ŒåŒ…å«cell state, hidden state\n",
    "def init_lstm_state(batch_size, hidden_units, device):\n",
    "    return (torch.zeros((batch_size, hidden_units), device=device), \n",
    "            torch.zeros((batch_size, hidden_units), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä¸€éƒ¨åˆ†è¿›è¡Œå„å‚æ•°çš„åˆå§‹åŒ–ï¼Œé¦–å…ˆå®šä¹‰ä¸€ä¸ª`normal`å‡½æ•°ç”¨äºç”Ÿæˆæ»¡è¶³æ­£æ€åˆ†å¸ƒçš„Tensorå½¢å¼çš„æ•°æ®ã€‚\n",
    "\n",
    "åé¢å†å®šä¹‰åŒ…å«é—å¿˜é—¨ï¼Œè¾“å…¥é—¨ï¼Œè¾“å‡ºé—¨ï¼Œå€™é€‰è®°å¿†å•å…ƒï¼Œéšè—å±‚/è¾“å‡ºå±‚çš„å‚æ•°ã€‚å¹¶ä½¿ç”¨ä¸€ä¸ªå‚æ•°å­—å…¸ç»Ÿä¸€ç®¡ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def initialize_parameters(vocab_size, hidden_units, device):\n",
    "    std = 0.01\n",
    "    input_units = output_units = vocab_size\n",
    "\n",
    "    # æ­£æ€åˆ†å¸ƒ\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * std\n",
    "\n",
    "    # LSTM cell weights\n",
    "    forget_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    input_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    output_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    c_tilda_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "\n",
    "    # åç½®é¡¹\n",
    "    forget_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    input_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    output_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    c_tilda_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "\n",
    "    # è¾“å‡ºå±‚å‚æ•°\n",
    "    hidden_output_weights = normal((hidden_units, output_units))\n",
    "    output_bias = torch.zeros((1, output_units), device=device)\n",
    "\n",
    "    # å°†æ‰€æœ‰å‚æ•°æ·»åŠ åˆ°å­—å…¸\n",
    "    parameters = {\n",
    "        'fgw': forget_gate_weights,\n",
    "        'igw': input_gate_weights,\n",
    "        'ogw': output_gate_weights,\n",
    "        'cgw': c_tilda_gate_weights,\n",
    "        'fgb': forget_gate_bias,\n",
    "        'igb': input_gate_bias,\n",
    "        'ogb': output_gate_bias,\n",
    "        'cgb': c_tilda_gate_bias,\n",
    "        'how': hidden_output_weights,\n",
    "        'ob': output_bias\n",
    "    }\n",
    "\n",
    "    # è®¾ç½® requires_grad=True ä»¥å¯ç”¨æ¢¯åº¦è®¡ç®—\n",
    "    # ç¡®ä¿æ‰€æœ‰å‚æ•°åœ¨åå‘ä¼ æ’­ä¸­èƒ½å¤Ÿè®¡ç®—æ¢¯åº¦\n",
    "    for param in parameters.values():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢è¿™ä¸€å—å°±å¼€å§‹ lstm çš„ä»£ç å®ç°äº†ï¼Œé˜…è¯»è¿™éƒ¨åˆ†æ—¶ç¡®ä¿ä½ å·²ç»å¯¹ç›¸å…³çš„å…¬å¼æœ‰æ‰€äº†è§£ã€‚\n",
    "\n",
    "åœ¨ä»£ç å†…éƒ¨ï¼Œæˆ‘ä»¬é¦–å…ˆä»å‚æ•°å­—å…¸ä¸­è¯»å–å‡ºç›¸å…³çš„å‚æ•°ï¼Œç„¶åå°†ä¼ å…¥çš„å½“å‰æ‰¹æ¬¡æ•°æ®ä¸å†å²çš„ hidden state è¿›è¡Œä¸²è”ã€‚æ¥ç€å†ä¾æ¬¡é€šè¿‡æ¯ä¸ª\"é—¨\"ï¼Œå¹¶ä¸”è®¡ç®—LSTM çš„ cell_state ä¸ hidden stateã€‚\n",
    "\n",
    "æœ‰çš„åŒå­¦å¯èƒ½å°±ä¼šæœ‰ç–‘é—®äº†ï¼Œä»£ç ä¸­å®ç°çš„å…¬å¼å’Œç†è®ºè®²è§£ä¸­çš„å…¬å¼ä¸å¤ªä¸€æ ·å‘€ï¼Œä»¥é—å¿˜é—¨ä¸ºä¾‹ï¼š\n",
    "\n",
    "ç†è®ºè®²è§£ä¸­çš„å…¬å¼æ˜¯ï¼š \n",
    "$F_t = \\sigma(X_t W_{xf} + H_{t-1} W_{hf} + b_f)$\n",
    "\n",
    "å¯¹äºé—å¿˜é—¨ç»“æ„åº”è¯¥æœ‰ä¸¤ä¸ªæƒé‡å‚æ•°çŸ©é˜µï¼Œ$W_{xf}$ ä¸ $W_{hf}$ï¼Œè€Œä»£ç ä¸­åªæœ‰ä¸€ä¸ªå‚æ•°çŸ©é˜µ `fgw`ï¼Œå¹¶ä¸”ç†è®ºå…¬å¼ä¸­ï¼Œæˆ‘ä»¬å°† $X_t$ ä¸ $H_{t-1}$ ä¸æƒé‡çŸ©é˜µç›¸ä¹˜åæ‰è¿›è¡Œçš„æ‹¼æ¥ï¼Œè€Œä»£ç ä¸­å´ç›´æ¥æ‹¼æ¥äº†ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬æ¥ä¸€èµ·åˆ†æä¸€ä¸‹è¿™ä¸ªé—®é¢˜\n",
    "\n",
    "ä»¥è‚¡ç¥¨é¢„æµ‹é—®é¢˜ä¸ºä¾‹ï¼Œæˆ‘ä»¬è¦ç”¨è¿‡å» 7 å¤©çš„è‚¡ç¥¨æ•°æ®æ¥é¢„æµ‹ç¬¬ 8 å¤©çš„è‚¡ç¥¨ä»·æ ¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¾“å…¥æ•°æ® $X_t$ å°±æ˜¯ä¸€ä¸ª `batch_size * 7` çš„å‘é‡ï¼Œå…¶ä¸­ `batch_size` è¡¨ç¤ºå½“å‰æ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡ï¼Œ7 è¡¨ç¤ºè¿‡å» 7 å¤©çš„è‚¡ç¥¨ä»·æ ¼ã€‚\n",
    "\n",
    "é‚£ä¹ˆå‡è®¾ $X_t$ æ˜¯ä¸€ä¸ª $batch_size$ * 7 çš„å‘é‡ ï¼Œ $H_{t-1}$æ˜¯ä¸€ä¸ª `batch_size * hidden_units` çš„å‘é‡ï¼Œé‚£ä¹ˆ $X_t$ ä¸ $H_{t-1}$ é€šè¿‡ `torch.cat` æˆ–è€… `np.concatenate` æ‹¼æ¥åå¾—åˆ°çš„æ–°å‘é‡æ˜¯ $batch_size * (7 + hidden_units)$ã€‚\n",
    "\n",
    "å¯¹äºæƒé‡çŸ©é˜µï¼Œç›¸åº”çš„ï¼Œæˆ‘ä»¬åœ¨ä¸Šä¸€ä¸ª`code cell`å®šä¹‰å…¶å½¢çŠ¶ä¸º`(input_units + hidden_units, hidden_units)`ï¼Œå…¶ä¸­ `input_units` å®é™…ä¸Šå°±æ˜¯ 7ï¼Œå¯ä»¥çœ‹å‡ºä¸Šè¿™é‡Œå®šä¹‰çš„æƒé‡çŸ©é˜µåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼Œ$W_{xf} ä¸ W_{hf}$ï¼Œå‰è€…å½¢çŠ¶ä¸º `input_units * hidden_units`ï¼Œåè€…å½¢çŠ¶ä¸º `hidden_units * hidden_units`\n",
    "\n",
    "è¿™æ ·ï¼ŒåŸå§‹çš„ä¸¤ä¸ªç‹¬ç«‹çš„çŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•è¿ç®—$X_t W_{xf} + H_{t-1} W_{hf}$å¯ä»¥è¢«å•ç‹¬é‡å†™æˆä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œå³ $X_t W_{xf} + H_{t-1} W_{hf} = concat\\_ dataset  W$ã€‚è¿™ç§é‡å†™ä¸ä»…ç®€åŒ–äº†è¡¨è¾¾å¼ï¼Œè¿˜ä½¿å¾—å®ç°æ›´åŠ é«˜æ•ˆï¼Œå› ä¸ºå¯ä»¥å°‘ç»´æŠ¤äº†ä¸€ä¸ªæƒé‡çŸ©é˜µï¼Œåœ¨è®¡ç®—ä¸Šæ›´ç®€æ´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO åŠ å…¬å¼\n",
    "\n",
    "# single lstm cell\n",
    "def lstm_cell(batch_dataset, prev_hidden_state, prev_cell_state, parameters):\n",
    "    # get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    cgw = parameters['cgw']\n",
    "\n",
    "    fgb = parameters['fgb']\n",
    "    igb = parameters['igb']\n",
    "    ogb = parameters['ogb']\n",
    "    cgb = parameters['cgb']\n",
    "    \n",
    "    # ä¸²è” data å’Œ prev_hidden_state  \n",
    "    # concat_dataset = torch.cat((batch_dataset, prev_hidden_state), dim=1)\n",
    "    concat_dataset = np.concatenate((batch_dataset, prev_hidden_state), axis=1)\n",
    "\n",
    "    # forget gate activations\n",
    "    F = sigmoid(np.matmul(concat_dataset, fgw) + fgb)\n",
    "\n",
    "    # input gate activations\n",
    "    I = sigmoid(np.matmul(concat_dataset, igw) + igb)\n",
    "\n",
    "    # output gate activations\n",
    "    O = sigmoid(np.matmul(concat_dataset, ogw) + ogb)\n",
    "\n",
    "    # cell_tilda gate activations\n",
    "    C_tilda = np.tanh(np.matmul(concat_dataset, cgw) + cgb)\n",
    "\n",
    "    # æ›´æ–° cell state, hidden_state\n",
    "    cell_state = F * prev_cell_state + I * C_tilda\n",
    "    hidden_state = np.multiply(O, np.tanh(cell_state))\n",
    "\n",
    "    # store four gate weights to be used in back propagation\n",
    "    lstm_activations = {\n",
    "        'F': F,\n",
    "        'I': I,\n",
    "        'O': O,\n",
    "        'C_tilda': C_tilda\n",
    "    }\n",
    "    \n",
    "    return lstm_activations, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾“å‡ºå±‚\n",
    "# éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåªæœ‰éšçŠ¶æ€æ‰ä¼šä¼ é€’åˆ°è¾“å‡ºå±‚ï¼Œè€Œè®°å¿†å…ƒä¸ç›´æ¥å‚ä¸è¾“å‡ºè®¡ç®—ï¼Œè®°å¿†å…ƒå®Œå…¨å±äºå†…éƒ¨ä¿¡æ¯\n",
    "def output_cell(hidden_state, parameters):\n",
    "    # get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    ob = parameters['ob']\n",
    "    # calculate the output\n",
    "    output = np.matmul(hidden_state, how) + ob\n",
    "    # å¦‚æœè¾“å‡ºä¸ºæ¦‚ç‡çš„è¯ï¼Œå¯ä»¥ä½¿ç”¨softmaxå‡½æ•°è¿›è¡Œå½’ä¸€åŒ–\n",
    "    # output = softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å®šä¹‰å®Œ `lstm_cell` ä»¥åŠ `output_cell` ä¹‹åï¼Œæˆ‘ä»¬åœ¨å…¶ä¹‹ä¸Šå®šä¹‰äº†ä¸€ä¸ª lstm ï¼Œè´Ÿè´£è¾“å…¥æ•°æ®å¹¶æ‹¿åˆ°è¾“å‡ºï¼Œ`lstm`å‡½æ•°åŒ…å«ä¸‰ä¸ªå‚æ•°ï¼Œ`batch_dataset` è¡¨ç¤ºä¸€æ‰¹è¾“å…¥æ•°æ®ï¼Œ`initial_state` è¡¨ç¤ºåˆå§‹åŒ–çŠ¶æ€çš„ä¸€ä¸ªå‡½æ•°ï¼Œparameters è¡¨ç¤ºå½“å‰æ¨¡å‹çš„å‚æ•°ã€‚\n",
    "\n",
    "æˆ‘ä»¬å…ˆåˆå§‹åŒ–æ¨¡å‹ stateï¼Œç„¶åä¾æ¬¡é€šè¿‡ `lstm_cell` æ‹¿åˆ°æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œæœ€åé€šè¿‡ `output_cell` æ‹¿åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(batch_dataset, initail_state, parameters):\n",
    "    hidden_state, cell_state = initail_state\n",
    "    outputs = []\n",
    "    _, hidden_state, cell_state = lstm_cell(batch_dataset, hidden_state, cell_state, parameters)\n",
    "    outputs.append(output_cell(hidden_state, parameters))    \n",
    "    return outputs, (hidden_state, cell_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åšå®Œä¸Šè¿°å·¥ä½œï¼Œæˆ‘ä»¬å·²ç»è§£å†³äº†å‰å‘ä¼ æ’­çš„é—®é¢˜ï¼Œè¿˜éœ€è¦å°è£…ä¸€ä¸ªç®€å•çš„ RNNç±»æ¥åšæ¨¡å‹å‚æ•°çš„åˆå§‹åŒ–ï¼ŒçŠ¶æ€çš„åˆå§‹åŒ–ä»¥åŠå‰å‘ä¼ æ’­ã€‚\n",
    "\n",
    "`__call__`æ–¹æ³•çš„ä½œç”¨æ˜¯ä½¿å®ä¾‹å¯¹è±¡å¯ä»¥åƒè°ƒç”¨æ™®é€šå‡½æ•°é‚£æ ·ï¼Œä»¥â€œå¯¹è±¡å()â€çš„å½¢å¼ä½¿ç”¨ã€‚å®Œæˆ lstm çš„å‰å‘ä¼ æ’­ `forward_fn`ï¼Œæˆ‘ä»¬åªéœ€è¦å°†å…¶æŒ‡å®šä¸ºä¸Šè¿°æ‰€å®šä¹‰çš„`lstm`å‡½æ•°å³å¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸€ä¸ªRNN ç±»æ¥è®­ç»ƒLSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModelScratch:\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.params = get_params(vocab_size, hidden_units, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        # æ ¹æ®ä»»åŠ¡ä¸åŒçµæ´»å¯¹è¾“å…¥æ•°æ®è¿›è¡Œé¢„å…ˆå¤„ç†\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        # X = X.type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RNNModelScratch at 0x155083130>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModelScratch(vocab_size, hidden_units, device, initialize_parameters, init_lstm_state, lstm)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å±•ç¤ºæ¨¡å‹çš„å‚æ•°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fgw': tensor([[-0.0129,  0.0029, -0.0024,  ..., -0.0031,  0.0026, -0.0097],\n",
       "         [-0.0105, -0.0057, -0.0095,  ...,  0.0121,  0.0177, -0.0036],\n",
       "         [ 0.0069,  0.0037, -0.0084,  ...,  0.0045,  0.0112,  0.0222],\n",
       "         ...,\n",
       "         [ 0.0178, -0.0045,  0.0048,  ..., -0.0028, -0.0112, -0.0143],\n",
       "         [-0.0027,  0.0034,  0.0107,  ..., -0.0064,  0.0085,  0.0130],\n",
       "         [-0.0070,  0.0030, -0.0072,  ...,  0.0096, -0.0125,  0.0097]],\n",
       "        requires_grad=True),\n",
       " 'igw': tensor([[ 0.0003,  0.0072, -0.0107,  ..., -0.0024, -0.0070, -0.0120],\n",
       "         [ 0.0151,  0.0166, -0.0284,  ..., -0.0058, -0.0040, -0.0056],\n",
       "         [-0.0079, -0.0048, -0.0048,  ..., -0.0017, -0.0081, -0.0022],\n",
       "         ...,\n",
       "         [-0.0239,  0.0221,  0.0033,  ...,  0.0129, -0.0150, -0.0046],\n",
       "         [-0.0017, -0.0003,  0.0045,  ..., -0.0064,  0.0023,  0.0043],\n",
       "         [-0.0104,  0.0113,  0.0076,  ...,  0.0147,  0.0108, -0.0085]],\n",
       "        requires_grad=True),\n",
       " 'ogw': tensor([[ 0.0052,  0.0051,  0.0007,  ..., -0.0007, -0.0156, -0.0086],\n",
       "         [-0.0079, -0.0078, -0.0069,  ...,  0.0171,  0.0044, -0.0043],\n",
       "         [ 0.0055,  0.0207, -0.0039,  ...,  0.0017, -0.0040, -0.0188],\n",
       "         ...,\n",
       "         [-0.0232,  0.0135,  0.0068,  ..., -0.0017, -0.0032,  0.0037],\n",
       "         [ 0.0122, -0.0045, -0.0130,  ...,  0.0177,  0.0129,  0.0033],\n",
       "         [ 0.0016,  0.0036, -0.0026,  ..., -0.0029,  0.0040,  0.0013]],\n",
       "        requires_grad=True),\n",
       " 'cgw': tensor([[ 0.0138,  0.0099, -0.0125,  ...,  0.0078, -0.0059,  0.0108],\n",
       "         [-0.0117, -0.0180, -0.0131,  ..., -0.0056, -0.0296,  0.0055],\n",
       "         [ 0.0009,  0.0170, -0.0059,  ...,  0.0026,  0.0200, -0.0031],\n",
       "         ...,\n",
       "         [ 0.0083, -0.0007, -0.0012,  ...,  0.0063,  0.0033,  0.0181],\n",
       "         [-0.0071, -0.0068,  0.0106,  ...,  0.0032, -0.0072,  0.0089],\n",
       "         [-0.0085,  0.0151, -0.0151,  ...,  0.0018,  0.0084,  0.0040]],\n",
       "        requires_grad=True),\n",
       " 'fgb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'igb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'ogb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'cgb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'how': tensor([[-0.0070,  0.0021,  0.0006,  ..., -0.0049,  0.0111, -0.0052],\n",
       "         [-0.0008, -0.0125, -0.0029,  ..., -0.0038, -0.0053, -0.0038],\n",
       "         [-0.0210, -0.0028,  0.0008,  ..., -0.0080, -0.0012,  0.0115],\n",
       "         ...,\n",
       "         [-0.0011,  0.0050,  0.0214,  ...,  0.0040,  0.0125,  0.0001],\n",
       "         [-0.0101, -0.0028, -0.0076,  ..., -0.0067, -0.0178, -0.0107],\n",
       "         [ 0.0133,  0.0018,  0.0018,  ..., -0.0031, -0.0073, -0.0072]],\n",
       "        requires_grad=True),\n",
       " 'ob': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]], requires_grad=True)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ­å–œä½ ğŸ‰**\n",
    "\n",
    "è‡³æ­¤ä½ å·²ç»èƒ½å®Œæˆ `lstm` æ¨¡å‹çš„å‰å‘ä¼ æ’­äº†ï¼Œ`model.py`çš„å†…å®¹ä¸æœ¬ç¯‡ä»£ç ç±»ä¼¼ã€‚åœ¨ä¸‹ä¸€ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å°†ä¼šé€šè¿‡ä¸€ä¸ªè‚¡ç¥¨é¢„æµ‹é—®é¢˜ï¼Œæ¥åº”ç”¨æˆ‘ä»¬çš„lstmæ¨¡å‹ï¼ŒåŒæ—¶åŠ å…¥åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°éƒ¨åˆ†ã€‚å…·ä½“ä»£ç åœ¨ `main.py`ä¸­ã€‚è¿è¡Œä¸‹åˆ—å‘½ä»¤å°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†\n",
    "\n",
    "ps: å› ä¸ºè¿‡ç¨‹ä¸­ä½¿ç”¨äº† `swanlab` è¿™ä¸ªå·¥å…·æ¥åšå¯è§†åŒ–ï¼Œæ‰€ä»¥æ‚¨éœ€è¦å…ˆæ³¨å†Œä¸€ä¸ªè´¦å·å¹¶åœ¨ç»ˆç«¯ä¸­è´´å…¥ä½ çš„ `api_key`\n",
    "\n",
    "swanlab: [swanlab.cn](https://swanlab.cn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3852.87s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<model.RNNModelScratch object at 0x132dcba30>\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.3.23                                  \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/Users/little1d/Desktop/Playground/LSTM-From-Scratch/notebook/swanlog/run-20241108_165219-2a23d349\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ‘‹ Hi \u001b[1m\u001b[39mHarrison\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mLSTM\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸŒŸ Run `\u001b[1mswanlab watch /Users/little1d/Desktop/Playground/LSTM-From-Scratch/notebook/swanlog\u001b[0m` to view SwanLab Experiment Dashboard locally\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction/runs/9ukh7j8s98w9sdw6y7sis\u001b[0m\u001b[0m\n",
      "Epoch 1, Loss: 0.2294756778412395\n",
      "Epoch 2, Loss: 0.011775485281961866\n",
      "Epoch 3, Loss: 0.002961122291000922\n",
      "Epoch 4, Loss: 0.002451772078832922\n",
      "Epoch 5, Loss: 0.0021021694523773882\n",
      "Epoch 6, Loss: 0.0018974299099580902\n",
      "Epoch 7, Loss: 0.0017400443749566977\n",
      "Epoch 8, Loss: 0.0016426904540922907\n",
      "Epoch 9, Loss: 0.0015705324921226646\n",
      "Epoch 10, Loss: 0.0015044413885334507\n",
      "Epoch 11, Loss: 0.0015101283578486699\n",
      "Epoch 12, Loss: 0.0014672611287096515\n",
      "Epoch 13, Loss: 0.0013940357619301518\n",
      "Epoch 14, Loss: 0.0013344537227466288\n",
      "Epoch 15, Loss: 0.0012397624283645807\n",
      "Epoch 16, Loss: 0.0012222831759976947\n",
      "Epoch 17, Loss: 0.00120571398777732\n",
      "Epoch 18, Loss: 0.0011475305089132031\n",
      "Epoch 19, Loss: 0.0011208955499265965\n",
      "Epoch 20, Loss: 0.00112965861788123\n",
      "Epoch 21, Loss: 0.0010640612397562815\n",
      "Epoch 22, Loss: 0.0010619352744672345\n",
      "Epoch 23, Loss: 0.0010453782055669257\n",
      "Epoch 24, Loss: 0.0010723455278720292\n",
      "Epoch 25, Loss: 0.0010933163535406089\n",
      "Epoch 26, Loss: 0.0010416477962280624\n",
      "Epoch 27, Loss: 0.00098799324930749\n",
      "Epoch 28, Loss: 0.0009662236973074161\n",
      "Epoch 29, Loss: 0.0009593831621006959\n",
      "Epoch 30, Loss: 0.0009458516764829659\n",
      "Epoch 31, Loss: 0.0009294575825656971\n",
      "Epoch 32, Loss: 0.0009261835334149913\n",
      "Epoch 33, Loss: 0.0009095736661240355\n",
      "Epoch 34, Loss: 0.000903696504893661\n",
      "Epoch 35, Loss: 0.0008960804625530727\n",
      "Epoch 36, Loss: 0.0008970295360389476\n",
      "Epoch 37, Loss: 0.0008677644830944095\n",
      "Epoch 38, Loss: 0.0008739470116173228\n",
      "Epoch 39, Loss: 0.0008506751659701371\n",
      "Epoch 40, Loss: 0.0008439103516543077\n",
      "Epoch 41, Loss: 0.0008311234907725722\n",
      "Epoch 42, Loss: 0.0008459278085663553\n",
      "Epoch 43, Loss: 0.0008125396517344699\n",
      "Epoch 44, Loss: 0.0008044609138677414\n",
      "Epoch 45, Loss: 0.0008081319105662664\n",
      "Epoch 46, Loss: 0.0007828941525076516\n",
      "Epoch 47, Loss: 0.000788869156773823\n",
      "Epoch 48, Loss: 0.0007756685073319306\n",
      "Epoch 49, Loss: 0.0007679181653656997\n",
      "Epoch 50, Loss: 0.0007690867261974037\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Experiment \u001b[33mLSTM\u001b[0m has completed\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸŒŸ Run `\u001b[1mswanlab watch /Users/little1d/Desktop/Playground/LSTM-From-Scratch/notebook/swanlog\u001b[0m` to view SwanLab Experiment Dashboard locally\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@Harrison/Google-Stock-Prediction/runs/9ukh7j8s98w9sdw6y7sis\u001b[0m\u001b[0m\n",
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
