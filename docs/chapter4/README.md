#  手搓一个Seq2Seq模型



在这个模块,将为大家详细讲解NLP入门必须要懂的Seq2Seq模型,注意力机制是什么以及如何将注意力机制融入到Seq2Seq模型中



## 模型分类

我们可以根据输入输出的结构来对传统NLP模型进行分类，大致可以分为以下四种类别：

1.  1 vs N
2.  N vs 1
3.  N vs N
4.  N vs M

其中，数字和字母表示模型输入或输出的词/字符数量。

他们分别对应的场景如下:

1. **1 vs N**（单输入，多输出）：文本生成(给一字,生成一篇对应主题的文章), 翻译任务(翻译一个词)
2. **N vs 1**（多输入，单输出）: 分类任务(给一段话,看看这段话输入什么类别)
3. **N vs N**（多输入，多输出）: 用来做古诗词续写(输入窗前明月光, 让模型输出下一句)
4. **N vs M**（多输入，多输出，长度不同）：这个就很通用了, 可用于文本生成, 翻译, 聊天对话等



我们现在用的最多的一类模型就是N vs M, 而Seq2Seq也就是这一类模型的典型代表之一。



## Seq2Seq介绍

Seq2Seq是一种多输入多输出的模型结构，适用于许多实际应用场景，是NLP领域最经典的模型之一。即使是在当前流行的Transformer框架中，我们依然能看到Seq2Seq的影子。下面是Seq2Seq的结构。

<div align=center>
<img src="images/Seq2SeqStruction.png" height = "200">
<p>Seq2Seq结构</p>
</div>



Seq2Seq由两个组件组成:

1. Encode层
2. Decode层

接下来我将为大家介绍这两个组件。



### Encode解码器

编码器负责将输入序列（例如，一句话的单词序列）逐步编码成一个固定大小的向量表示。编码器通常由一系列递归神经网络（如RNN,LSTM或GRU）层组成，逐个处理输入序列的元素，并通过隐藏状态来逐步积累上下文信息。最后一个隐藏状态包含了整个输入序列的信息，作为压缩后的输入序列表示，这一向量会传递给解码器。

<div align=center>
<img src="images/image-20241103212541411.png" height = "300">
<p>Encode结构</p>
</div>



如图所示，这是一个编码器结构，由嵌入层和循环神经网络组成。它的输入是一句话，输出为一个n维向量，用于表示该句子的语义。整个过程被称为编码，具体步骤如下：

1. 对句子进行分词。
2. 构建词汇表。
3. 将句子中的每个词转化为对应的词汇表序号。
4. 通过Embedding层将这些序号映射为向量。
5. 将向量输入到循环神经网络中进行信息提取。
6. 最后，输出一个隐藏状态编码 c，作为句子的语义表示。



下面是一个具体例子:

假设输入句子为“我爱学习”。

1. **分词**：将句子分成“what”、“time”、“is”、“it”。
2. **构建词汇表**：假设词汇表中“what”为1，“time”为2，“is”为3，“it”为4。
3. **转换为序号**：句子转化为序号序列1,2,3,4。
4. **Embedding映射**：通过Embedding层，将序号1,2,3,4映射为向量序列。
5. **循环神经网络**：将向量序列输入RNN/LSTM/GRU提取信息。
6. **生成编码**：得到的隐藏状态编码 c,  即为“我爱学习”的语义表示。



关于最后隐藏状态 c 所包含的语义，可以参考下图。最后的 c 就是包含句子 "what time is it" 的语义向量。

<div align=center>
<img src="images/image-20241103214423250.png" height = "300">
</div>





### Decode解码器

解码器的主要作用是根据目标生成我们期望的结果。它通过分析输入信息，逐步生成符合目标要求的输出内容，确保生成的结果与预期保持一致。结构如下:

<div align=center>
<img src="images/image-20241103215152904.png" height = "300">
<p>Decode结构</p>
</div>

解码器的输入包括三个部分：编码器最终生成的隐藏语义向量 c、开始符号的向量，以及初始化的中间语义。最终输出则是我们想要的目标结果。以图中示例为例，最终输出是句子 "欢迎来北京" 的英语翻译。整个过程称为解码。具体步骤如下：

1. 对输入句子进行编码，得到语义向量 c。
2. 构建一个输入起始符（例如，" |start|" 符号）。
3. 对起始符号进行Embedding处理。
4. 初始化一个中间语义 $S_0$。
5. 将中间语义和 c 拼接，得到整体的隐含语义输出。
6. 在每一步中，GRU 接收上一个时间步的输出、上一个时间步的中间语义，以及 c 拼接后的语义。
7. 最终，将每个步骤的输出拼接在一起，形成解码结果。



以上就是整个解码器的运作过程。





## 注意力机制

当我们观察事物时，之所以能快速做出判断（即便这种判断可能存在误差），是因为大脑能够迅速将注意力聚焦于事物最具辨识度的部分，而不是先全面观察事物的每个细节后才得出结论。正是基于这种机制，注意力机制理论应运而生。

<div align=center>
<img src="images/image-20241104232626422.png" height = "300">
</div>

这就像当你看到上面这幅图片时，第一眼吸引你注意的部分可能各不相同：有些人第一眼关注的是图中的角色，而另一些人则可能注意到其中的花朵。每个人的注意力机制都有其独特之处。而如何训练出我们期望的注意力机制（例如让所有人第一眼看到这张图片就关注到其中的角色）也是当前研究的一个重点方向。



注意力机制的计算公式多种多样，常见的有两种: 

1.   加性注意力机制
2.   点积注意力机制



### 加性注意力机制

在Transformer框架出来前,人们大多数用的是加性注意力机制, 这种注意力机制的思想是通过线性层和隐藏单元的线性变化来调整维度, 使得变成一个权重矩阵, 公式如下:

注意力分数计算如下

$$
score(s_t, h_i) = v_a^T \tanh(W_a s_t + U_a h_i)
$$


注意力权重计算

$$
\alpha_{ti} = \frac{\exp(score(s_t, h_i))}{\sum_{j=1}^T \exp(score(s_t, h_j))}
$$

上下文计算

$$
c_t = \sum_{i=1}^T \alpha_{ti} h_i
$$

1.  $s_t$是解码器在时间步t的隐状态
2.  $h_i$ 是编码器输出的隐状态
3.  $W_a$和 $U_a$是可训练的参数矩阵
4.  $v_a$ 是可学习的权重向量



上面的注意力公式可以这样理解:

我们把编码器经过编码后得到的隐藏状态 $h_t$ 和解码器上一个时间步的隐藏状态 $s_t$ 拼接在一起,然后通过一个可训练的参数矩阵来调节这个隐藏状态的形状吗, 让它变成 $1*N$ 的注意力分数向量, 然后归一化,变成权重矩阵。最后和 $h_t$ 做加权,得到下一个时间步的隐藏状态单元输入。

这就好比我们在做翻译任务的时候, 我们将编码器输出的带有整个中文句子信息的隐藏状态 $h_t$ 和解码器上一个时间步的隐藏状态 $s_t$ 给拼接在一起,然后扔给一个参数向量, 这个参数向量就会调节这些信息, 使之变成一个权重向量, 这个权重向量将告诉解码器,下一个时间步中,我们要将多少注意力集中在哪个部分上面。







### 点积注意力机制

点积注意力则是2017年论文《Attention Is All You Need》中所提出的注意力计算方式。该公式凭借其强大的并行计算优势，从众多公式中脱颖而出，成为领域内的重要基石。

公式如下:

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$


- **$Q$（Query）**：查询矩阵，用于代表当前单词的特征。
- **$K$（Key）**：键矩阵，代表序列中所有单词的特征。
- **$V $（Value）**：值矩阵，包含需要提取的信息。
- **$$d_k$$**：键矩阵 $K$ 的维度，用于缩放，避免数值过大导致梯度消失问题。
- **$Softmax$**：用于归一化，将结果转化为概率分布，以便加权求和。



下面的例子说明这个公式的作用:

可以把注意力机制的公式理解为一个“寻找最相关信息的过程”。想象一下，你在看一本书，而这本书的每一页上都有很多单词。但你在阅读时并不是每个单词都仔细看，而是会自动地把注意力集中在最重要的内容上，来快速理解书的核心思想。这个过程可以用注意力机制来形象地说明。

假设：

- **$Q$（Query）** 是你现在想了解的一个问题，比如“这本书的主要情节是什么？”
- **$K$（Key）** 是这本书每一页的内容摘要，表示书中每一页的信息。
- **$V$（Value）** 是这本书的实际内容。
- **$Softmax$** 就是你的大脑在每一页的摘要中找到与问题（Query）最相关的页面，并把注意力集中在这些页面上。

工作过程如下：

1. 你先带着“想知道主要情节”的问题（Query）去看书。
2. 你快速扫一眼每一页的摘要（Key），看看哪些摘要最符合你的问题。
3. 当你找到最相关的页面时，你的大脑会特别关注这些页面的实际内容（Value）。
4. Softmax 类似于你的大脑在权衡每一页的相关性，将注意力更多地分配给那些与你的问题高度相关的页面，而忽略不太相关的页面。

这样一来，通过注意力机制，你可以在不通读全书的情况下，快速抓住书中的主要情节。这就是注意力机制的基本工作原理：在一堆信息中，自动找到和问题最相关的部分，进而加以关注。



## 注意力机制与Seq2Seq的结合

以一个翻译场景为例，当我们翻译一段中文文本成英文时，通常只会关注中文句子中与当前翻译词相关的部分。比如在翻译"I"时，我们的注意力会集中在"我"这个词上；而当翻译"love"时，我们会关注"我爱..."的部分，以确定"love"的正确时态和语气。如果在翻译"I"时，我们需要查看整句中文，可能会因为信息量过大而无法有效找出关键点，导致翻译不准确。类似地，机器翻译也存在这个问题。当模型在翻译某个词时，注意力机制能够动态调整解码器的关注焦点，确保它只关注与当前输出相关的输入部分。这不仅提升了翻译的准确性，还更好地保留了原句的语义。

在Seq2Seq模型中引入加性注意力机制时，我们主要关注三个关键要素：

* **解码器状态**：表示当前翻译进度的向量 $h_t$，它包含了已经翻译出的内容信息
* **编码器状态集合**：源句子中每个词的编码向量集合 ${h_1, h_2, ..., h_n}$
* **对齐关系**：计算当前解码状态与每个编码状态的相关程度

加性注意力的计算过程如下：

1. 首先，我们有一个打分函数 $score(h_t, h_s)$ ，用于计算解码器状态 $h_t$ 和编码器状态 $h_s$ 之间的相关性：
   $score(h_t, h_s) = v_a^T * tanh(W_a * h_t + U_a * h_s)$

   其中：
   -  $W_a$ 和 $U_a$ 是可学习的权重矩阵
   - $v_a$ 是可学习的权重向量
   - $tanh$ 是激活函数

2. 对所有位置的分数进行 $softmax$ 归一化，得到注意力权重：
   $α_ts = softmax(score(h_t, h_s))$

3. 最后，将所有编码器状态按注意力权重加权求和，得到上下文向量：
   $c_t = Σ(α_ts * h_s)$

以翻译"我爱你"为例,当解码器要生成"love"时:
1. 首先有解码器当前状态(包含了"I"的信息)
2. 计算该状态与输入句子(即隐藏状态和 $h_t$ )中每个位置的相关性分数
3. 通过 $softmax$ 得到的权重可能是:[0.1, 0.7, 0.2]，表示模型主要关注"爱"这个词
4. 根据这些权重对编码器状态进行加权组合，得到上下文向量
5. 将上下文向量和当前解码器状态一起用于预测"love"



具体示意图



<div align=center>
<img src="images/image-20241112232044542.png" height = "300">
</div>



1. 红色箭头第一步是将上一个时间步的隐藏状态单元和Encoder输出的隐藏向量做点积和缩放
2. 蓝色箭头第二步就是对上一步点积的值做softmax归一化, 变成权重
3. 绿色箭头第三步就是让编码层的隐藏状态单元与权重向量结合,得到带权重的信息,作为decoder下一个时间步隐藏状态的输入

