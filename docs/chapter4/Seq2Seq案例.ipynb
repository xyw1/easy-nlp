{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed797b82-b4f3-4538-8d12-9e7daf052432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from io import open\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047428d-d45c-4a5b-8b72-8646e7081f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义开始符号和结束符号的常量\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "MAX_LENGTH = 10  # 定义句子的最大长度\n",
    "\n",
    "# 语言类，负责处理语言相关操作，如创建词汇表\n",
    "class Lang:\n",
    "    def __init__(self, language):\n",
    "        self.language = language  # 保存语言的名称\n",
    "        self.size = 2  # 初始词汇表大小为2（SOS和EOS）\n",
    "        self.word2index = {}  # 单词到索引的映射\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}  # 索引到单词的映射，初始包含SOS和EOS\n",
    "\n",
    "    # 添加单词到词汇表\n",
    "    def AddWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.size  # 为新单词分配索引\n",
    "            self.index2word[self.size] = word  # 保存索引到单词的映射\n",
    "            self.size += 1  # 增加词汇表的大小\n",
    "\n",
    "    # 添加句子中的所有单词到词汇表\n",
    "    def AddSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.AddWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc513a-4267-4b9f-9cfb-10ecaf93e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符规范化\n",
    "def unicodeToAscii(text):\n",
    "    # 将Unicode字符转换为ASCII字符, 同时去掉重音符号，将é转换为e\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(text):\n",
    "    # 这个函数确保输入的文本只包含标准的ASCII字符\n",
    "    # 防止多种编码格式导致不必要的错误\n",
    "    s = unicodeToAscii(text.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643339b-94eb-458f-b306-0879287a023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读取数据并进行初步处理\n",
    "def readLangs(path, input_lang, output_lang):\n",
    "    data = open(path, encoding='utf-8').read().split('\\n') \n",
    "    pairs = [[normalizeString(text) for text in l.split('\\t')] for l in data]  # 将每行中的句子分割并规范化\n",
    "    input_lang = Lang(input_lang)  # 创建输入语言类\n",
    "    output_lang = Lang(output_lang)  # 创建输出语言类\n",
    "    return input_lang, output_lang, pairs  # 返回输入和输出语言类以及句子对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c5d2f-2574-49db-aa1b-3b17a8322b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取英语和法语数据\n",
    "path = 'data/eng-fra.txt'  # 数据文件路径\n",
    "input_lang, output_lang, pairs = readLangs(path, \"eng\", \"fra\")\n",
    "\n",
    "# 由于数据集比较大,我们只取以下面开头的句子作为数据集来演示\n",
    "eng_prefixes = (\"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55a915-aad0-4b4c-a670-387959386182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据过滤，定义只保留长度小于MAX_LENGTH且符合特定前缀的英语句子\n",
    "def filterPair(p):\n",
    "    return len(p[0].split()) < MAX_LENGTH and any(p[0].startswith(prefix) for prefix in eng_prefixes) and len(p[1].split()) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# 数据预处理，包括创建词汇表和句子过滤\n",
    "def preprocess(path, input_lang, output_lang):\n",
    "    input_lang, output_lang, pairs = readLangs(path, input_lang, output_lang)  # 读取语言和句子对\n",
    "    pairs = filterPairs(pairs)  # 过滤不符合条件的句子对\n",
    "    for pair in pairs:\n",
    "        input_lang.AddSentence(pair[0])  # 添加输入句子中的单词到词汇表\n",
    "        output_lang.AddSentence(pair[1])  # 添加输出句子中的单词到词汇表\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = preprocess(path, 'eng', 'fra')  # 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8230865-f586-4757-afb9-2f458086e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子转换为张量表示\n",
    "def TensorFormSentence(Lang, sentence):\n",
    "    indexes = [Lang.word2index[word] for word in sentence.split(' ')]  # 将每个单词转换为对应的索引\n",
    "    indexes.append(EOS_TOKEN)  # 添加结束符号\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)  # 返回形状为 (句子长度, 1) 的张量\n",
    "\n",
    "# 将句子对转换为张量表示\n",
    "def TensorFormPairs(input_lang, output_lang, pair):\n",
    "    input_tensor = TensorFormSentence(input_lang, pair[0])  # 转换输入句子为张量\n",
    "    output_tensor = TensorFormSentence(output_lang, pair[1])  # 转换输出句子为张量\n",
    "    return (input_tensor, output_tensor)  # 返回输入和输出张量对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c7d59ec-785c-4e5d-880e-dfd6683b22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            input_size, hidden_size, device=device\n",
    "        )  # 词嵌入层\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, device=device)  # GRU层\n",
    "\n",
    "    def forward(self, input_tensor, hidden):\n",
    "        embedded = self.embedding(input_tensor).view(1, 1, -1)  # (1, 1, hidden_size)\n",
    "        output, hidden = self.gru(\n",
    "            embedded, hidden\n",
    "        )  # (1, 1, hidden_size) 和 (1, 1, hidden_size)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(\n",
    "            1, 1, self.hidden_size, device=device\n",
    "        )  # 初始化隐藏状态 (1, 1, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "436a549a-2285-409c-8862-96f96f6c89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带注意力机制的解码器\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_size, hidden_size, device=device\n",
    "        )  # 词嵌入层\n",
    "        self.attn = nn.Linear(2 * hidden_size, max_length).to(device)  # 计算注意力权重\n",
    "        self.attn_combine = nn.Linear(2 * hidden_size, hidden_size).to(\n",
    "            device\n",
    "        )  # 合并嵌入向量和注意力加权值\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size).to(device)  # GRU层\n",
    "        self.dropout = nn.Dropout(dropout).to(device)  # Dropout层\n",
    "        self.linear = nn.Linear(hidden_size, output_size).to(device)  # 输出层\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(input).view(1, 1, -1)\n",
    "        )  # (1, 1, hidden_size)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), dim=1)), dim=1\n",
    "        )  # (1, max_length)\n",
    "\n",
    "        # 计算加权后的上下文向量\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)\n",
    "        )  # (1, 1, hidden_size)\n",
    "\n",
    "        # 拼接嵌入向量和上下文向量\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)  # (1, 2 * hidden_size)\n",
    "        output = self.attn_combine(output).unsqueeze(0)  # (1, 1, hidden_size)\n",
    "        output = F.relu(output)  # (1, 1, hidden_size)\n",
    "\n",
    "        output, hidden = self.gru(\n",
    "            output, hidden\n",
    "        )  # (1, 1, hidden_size) 和 (1, 1, hidden_size)\n",
    "        output = self.linear(output[0])  # (1, output_size)\n",
    "        output = F.log_softmax(output, dim=1)  # (1, output_size)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea0aa9f9-36ab-415e-bfa4-603bb23527a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq模型\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor, teacher_forcing_ratio=0.5):\n",
    "        input_length = input_tensor.size(0)  # 输入序列的长度\n",
    "        target_length = target_tensor.size(0)  # 目标序列的长度\n",
    "\n",
    "        encoder_hidden = self.encoder.init_hidden()  \n",
    "        encoder_outputs = torch.zeros(\n",
    "            MAX_LENGTH, self.encoder.hidden_size, device=device\n",
    "        )  \n",
    "\n",
    "        # 编码阶段\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                input_tensor[ei], encoder_hidden\n",
    "            )  # (1, 1, hidden_size) 和 (1, 1, hidden_size)\n",
    "            encoder_outputs[ei] = encoder_output[\n",
    "                0, 0\n",
    "            ]  # 取出每个时间步的输出 (MAX_LENGTH, hidden_size)\n",
    "\n",
    "        # 初始化解码器输入（开始符号）和隐藏状态\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # (1, 1)\n",
    "        decoder_hidden = encoder_hidden \n",
    "        # (target_length, output_size)\n",
    "        all_decoder_outputs = torch.zeros(\n",
    "            target_length, self.decoder.output_size, device=device\n",
    "        )  \n",
    "\n",
    "        use_teacher_force = random.random() < teacher_forcing_ratio  # 是否使用教师强制\n",
    "\n",
    "        # 解码阶段\n",
    "        for di in range(target_length):\n",
    "            # (1, output_size), (1, 1, hidden_size), (1, max_length)\n",
    "            decoder_output, decoder_hidden, attn_weights = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )  \n",
    "            # 存储每一步的输出 (target_length, output_size)\n",
    "            all_decoder_outputs[di] = (\n",
    "                decoder_output \n",
    "            )\n",
    "            # 获取最大概率的词索引\n",
    "            topv, topi = decoder_output.topk(1) \n",
    "            # 获取下一个时间步的输入 (1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            # 使用真实标签作为下一步的输入\n",
    "            if use_teacher_force:\n",
    "                decoder_input = target_tensor[di]  \n",
    "        # (target_length, output_size)\n",
    "        return all_decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1286c63-636b-415e-ac8f-9fd597b22506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练函数\n",
    "def train(input_tensor, output_tensor, model, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=0.5):\n",
    "    encoder_optimizer.zero_grad()  # 梯度清零\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = output_tensor.size(0)\n",
    "    loss = 0\n",
    "\n",
    "    decoded_words = model(input_tensor, output_tensor, teacher_forcing_ratio)  # 使用Seq2Seq模型进行前向传播\n",
    "\n",
    "    # 计算损失\n",
    "    for di in range(output_length):\n",
    "        target_word = output_tensor[di]  # 获取目标词\n",
    "        output_word = decoded_words[di].unsqueeze(0)  # 使维度变为 (1, output_size)\n",
    "        loss += criterion(output_word, target_word)  # 计算损失\n",
    "\n",
    "    loss.backward()  # 反向传播\n",
    "\n",
    "    encoder_optimizer.step()  # 更新参数\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / output_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae13f28-bd62-4f5f-b0ae-36b6ba833662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练迭代器\n",
    "def trainIters(model, n_iters, learning_rate=0.01, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_loss_total = 0\n",
    "    total_loss = 0\n",
    "    encoder_optimizer = optim.SGD(model.encoder.parameters(), lr=learning_rate)  # 使用随机梯度下降优化器\n",
    "    decoder_optimizer = optim.SGD(model.decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()  # 损失函数使用负对数似然损失\n",
    "\n",
    "    tbar = tqdm(range(n_iters), desc='epoch', leave=False)\n",
    "\n",
    "    for epoch in tbar:\n",
    "        pair = random.choice(pairs)  # 随机选择一个句子对\n",
    "        input_tensor = TensorFormSentence(input_lang, pair[0])\n",
    "        output_tensor = TensorFormSentence(output_lang, pair[1])\n",
    "\n",
    "        loss = train(input_tensor, output_tensor, model, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "        plot_loss_total += loss\n",
    "        total_loss += loss\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_loss_total = 0\n",
    "            tbar.set_postfix(loss=f\"{total_loss / (epoch + 1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf32b9-8cea-4a3c-8fc8-335dd5aefc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "# 定义 Seq2Seq 模型\n",
    "hidden_size = 256\n",
    "encoder1 = Encoder(input_lang.size, hidden_size).to(device)  # 初始化编码器\n",
    "attn_decoder1 = AttentionDecoder(hidden_size, output_lang.size, max_length=10, dropout=0.1).to(device)  # 初始化带注意力机制的解码器\n",
    "seq2seq_model = Seq2Seq(encoder1, attn_decoder1).to(device)  # 创建Seq2Seq模型\n",
    "\n",
    "# 训练模型\n",
    "trainIters(seq2seq_model, n_iters=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69eebb5-71ae-4f90-8671-4bbe7b977d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    # 评估时不计算梯度\n",
    "    with torch.no_grad(): \n",
    "        # 将输入句子转换为张量\n",
    "        input_tensor = TensorFormSentence(input_lang, sentence)  \n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        # 初始化编码器隐藏状态\n",
    "        encoder_hidden = encoder.init_hidden()  \n",
    "        # 初始化编码器输出\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)  \n",
    "\n",
    "        # 编码器过程\n",
    "        for ei in range(input_length):\n",
    "            # 计算编码器的输出\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)  \n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "         # 解码器的输入初始化为SOS符号\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device) \n",
    "        # 解码器的隐藏状态初始化为编码器的隐藏状态\n",
    "        decoder_hidden = encoder_hidden  \n",
    "        # 存储解码器生成的单词\n",
    "        decoder_words = []  \n",
    "        # 存储注意力权重\n",
    "        decoder_attentions = torch.zeros(max_length, max_length, device=device)  \n",
    "\n",
    "        # 解码器过程\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)  # 解码每个时间步\n",
    "            decoder_attentions[di] = decoder_attention.data  # 保存注意力权重\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)  # 获取概率最高的单词索引\n",
    "            if topi.item() == EOS_TOKEN:  # 如果是结束符，停止解码\n",
    "                decoder_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoder_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # 将预测的单词作为下一个时间步的输入\n",
    "\n",
    "        return decoder_words, decoder_attentions[:di+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f72042-0d6e-4181-8a93-c381106b2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入> i m looking forward to seeing you .\n",
      "输出> j ai hate de vous voir .\n",
      "预测结果> je me de de te . . <EOS>\n",
      "--------------------------------------------------\n",
      "输入> he is what we call a scholar .\n",
      "输出> il est ce qu on appelle un lettre .\n",
      "预测结果> c est un homme un homme . . <EOS>\n",
      "--------------------------------------------------\n",
      "输入> he is enrolled at that university .\n",
      "输出> il est entre a cette universite .\n",
      "预测结果> il est en train de . . <EOS>\n",
      "--------------------------------------------------\n",
      "输入> i m certain .\n",
      "输出> je suis sur .\n",
      "预测结果> je suis en . . <EOS>\n",
      "--------------------------------------------------\n",
      "输入> she is always fishing for compliments .\n",
      "输出> elle est toujours en quete de compliments .\n",
      "预测结果> elle a la le . . . <EOS>\n",
      "--------------------------------------------------\n",
      "输入> he is popular with everybody .\n",
      "输出> il est populaire aupres de tout le monde .\n",
      "预测结果> il est le de de . . . <EOS>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 随机选择句子对并进行评估\n",
    "def evaluateRandomly(encoder, decoder, n=6):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)  # 随机选择一个句子对\n",
    "        print(f'输入> {pair[0]}')\n",
    "        print(f'输出> {pair[1]}')\n",
    "        output_words, output_attn = evaluate(encoder, decoder, pair[0])  # 评估模型的输出\n",
    "        output_sentence = ' '.join(output_words)  # 将生成的单词拼接为句子\n",
    "        print(f'预测结果> {output_sentence}')\n",
    "        print('-' * 50)\n",
    "\n",
    "# 调用评估函数\n",
    "evaluateRandomly(encoder1, attn_decoder1)  # 评估模型性能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e27aaf-340d-400e-a37e-0aa7bb830e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
