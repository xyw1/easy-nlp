{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8161bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2835a38",
   "metadata": {},
   "source": [
    "GRU（门控循环单元）是一种循环神经网络（RNN）的变体，它通过引入更新门（Update Gate）和重置门（Reset Gate）来控制信息的流动，从而解决了传统RNN中的梯度消失和梯度爆炸问题。GRU的设计使得它在处理序列数据时更加高效，尤其是在长序列数据上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915aeab",
   "metadata": {},
   "source": [
    "GRU主要包括四个部分：\n",
    "\n",
    "重置门（Reset Gate）：重置门控制着上一时间步的信息在多大程度上影响当前时间步的候选隐藏状态。其计算公式为：\n",
    "$$R^t = \\sigma(X^t W^{xr} + H^{t-1} W^{hr} + b^r)$$\n",
    "\n",
    "更新门（Update Gate）：更新门决定了上一时间步的隐藏状态在当前时间步的保留程度。其计算公式为：\n",
    "$$Z^t = \\sigma(X^t W^{xz} + H^{t-1} W^{hz} + b^z)$$\n",
    "\n",
    "候选隐藏状态（Candidate Hidden State）：候选隐藏状态结合了当前输入和上一时间步的信息，计算公式为：\n",
    "$$\\tilde{H}^{t}=\\tanh\\left(X^{t} W^{x h}+\\left(R^{t}\\odot H^{t-1}\\right) W^{h h}+b^{h}\\right)$$\n",
    "\n",
    "隐藏状态更新：最终的隐藏状态是上一时间步的隐藏状态和候选隐藏状态的加权和，计算公式为：\n",
    "$$H^{t} = Z^{t} \\odot H^{t-1} + (1 - Z^{t}) \\odot \\tilde{H}^{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513df552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义的GRU层\n",
    "class CostomGRU_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CostomGRU_layer, self).__init__()\n",
    "        # 初始化参数\n",
    "        self.W_xz = nn.Parameter(torch.randn(input_size, hidden_size))  # 更新门的输入到隐藏层的权重\n",
    "        self.W_hz = nn.Parameter(torch.randn(hidden_size, hidden_size))  # 更新门的隐藏层到隐藏层的权重\n",
    "\n",
    "        self.W_xr = nn.Parameter(torch.randn(input_size, hidden_size))  # 重置门的输入到隐藏层的权重\n",
    "        self.W_hr = nn.Parameter(torch.randn(hidden_size, hidden_size))  # 重置门的隐藏层到隐藏层的权重\n",
    "\n",
    "        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size))  # 候选隐藏状态的输入到隐藏层的权重\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))  # 候选隐藏状态的隐藏层到隐藏层的权重\n",
    "\n",
    "        self.hb_z = nn.Parameter(torch.zeros(hidden_size))  # 更新门的偏置\n",
    "        self.hb_r = nn.Parameter(torch.zeros(hidden_size))  # 重置门的偏置\n",
    "        self.hb_h = nn.Parameter(torch.zeros(hidden_size))  # 候选隐藏状态的偏置\n",
    "        \n",
    "        self.xb_z = nn.Parameter(torch.zeros(hidden_size))  \n",
    "        self.xb_r = nn.Parameter(torch.zeros(hidden_size))  \n",
    "        self.xb_h = nn.Parameter(torch.zeros(hidden_size))  \n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # 前向传播\n",
    "        z = torch.sigmoid((torch.matmul(x, self.W_xz) + self.xb_z) + (torch.matmul(h, self.W_hz) + self.hb_z))  # 更新门\n",
    "        r = torch.sigmoid((torch.matmul(x, self.W_xr) + self.xb_r) + (torch.matmul(h, self.W_hr) + self.hb_r))  # 重置门\n",
    "        h_tilda = torch.tanh((torch.matmul(x, self.W_xh) + self.xb_h) + r * (torch.matmul(h, self.W_hh) + self.hb_h))  # 候选隐藏状态\n",
    "        h = z * h + (1 - z) * h_tilda  # 更新隐藏状态\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e6b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义的GRU模型\n",
    "class CostomGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CostomGRU, self).__init__()\n",
    "        self.input_size = input_size  # 输入特征的维度\n",
    "        self.hidden_size = hidden_size  # 隐藏层的维度\n",
    "        # 初始化自定义的GRU层\n",
    "        self.gru = CostomGRU_layer(self.input_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, X, h0=None):\n",
    "        # x.shape = (batch_size, seq_length, input_size)\n",
    "        # h0.shape = (1, batch_size, hidden_size)\n",
    "        # output.shape = (seq_length, batch_size, hidden_size)\n",
    "\n",
    "        # 获取批次大小\n",
    "        batch_size = X.shape[1]\n",
    "        # 获取序列长度\n",
    "        self.seq_length = X.shape[0]\n",
    "        \n",
    "        # 如果没有提供初始隐藏状态，则初始化为零张量\n",
    "        if h0 is None:\n",
    "            prev_h = torch.zeros([batch_size, self.hidden_size]).to(device)\n",
    "        else:\n",
    "            prev_h = torch.squeeze(h0, 0) \n",
    "\n",
    "        # 初始化输出张量\n",
    "        output = torch.zeros([self.seq_length, batch_size, self.hidden_size]).to(device)\n",
    "\n",
    "        # 循环处理序列中的每个时间步\n",
    "        for i in range(self.seq_length):\n",
    "            # 通过GRU层处理当前时间步的数据，并更新隐藏状态\n",
    "            prev_h = self.gru(X[i], prev_h)\n",
    "            # 将当前时间步的输出存储在输出张量中\n",
    "            output[i] = prev_h\n",
    "\n",
    "        # 返回最终的输出和隐藏状态\n",
    "        return output, torch.unsqueeze(prev_h, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6e4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_length = 30\n",
    "input_size = 32\n",
    "hidden_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24df53c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 64])\n",
      "torch.Size([1, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(seq_length, batch_size, input_size).to(device)  # 创建一个随机输入张量，形状为(seq_length, batch_size, input_size)\n",
    "model = CostomGRU(input_size, hidden_size).to(device)  # 实例化自定义的GRU模型\n",
    "output = model(x)  # 进行前向传播\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(output[0].shape)  \n",
    "print(output[1].shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "790ac13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1 == output2 ? True\n",
      "h1 == h2 ? True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"测试：\n",
    "    将nn.GRU中的4个随机初始化的可学习参数进行保存，并替换掉\n",
    "      CostomGRU中CostomGRU_layer随机初始化的可学习参数，并通过torch.allclose\n",
    "      判断输出是否相等，若相等则证明MyGRU的实现与官方的nn.GRU是一致的\n",
    "\"\"\"\n",
    "\n",
    "# 初始化nn.GRU\n",
    "gru = nn.GRU(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "weight_ih_l0 = gru.weight_ih_l0.T\n",
    "weight_hh_l0 = gru.weight_hh_l0.T\n",
    "bias_ih_l0 = gru.bias_ih_l0\n",
    "bias_hh_l0 = gru.bias_hh_l0\n",
    "\n",
    "# 初始化CostomGRU\n",
    "costom_gru = CostomGRU(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# 替换CostomGRU中的参数\n",
    "costom_gru.gru.W_xr = nn.Parameter(weight_ih_l0[:, :costom_gru.gru.W_xr.size(1)])  # 更新门的输入权重\n",
    "costom_gru.gru.W_hr = nn.Parameter(weight_hh_l0[:, :costom_gru.gru.W_hr.size(1)])  # 更新门的隐藏权重\n",
    "\n",
    "costom_gru.gru.W_xz = nn.Parameter(weight_ih_l0[:, costom_gru.gru.W_xr.size(1):costom_gru.gru.W_xr.size(1) + costom_gru.gru.W_xz.size(1)])  # 重置门的输入权重\n",
    "costom_gru.gru.W_hz = nn.Parameter(weight_hh_l0[:, costom_gru.gru.W_hr.size(1):costom_gru.gru.W_hr.size(1) + costom_gru.gru.W_hz.size(1)])  # 重置门的隐藏权重\n",
    "\n",
    "costom_gru.gru.W_xh = nn.Parameter(weight_ih_l0[:, costom_gru.gru.W_xr.size(1) + costom_gru.gru.W_xz.size(1):])  # 候选隐藏状态的输入权重\n",
    "costom_gru.gru.W_hh = nn.Parameter(weight_hh_l0[:, costom_gru.gru.W_hr.size(1) + costom_gru.gru.W_hz.size(1):])  # 候选隐藏状态的隐藏权重\n",
    "\n",
    "costom_gru.gru.hb_r = nn.Parameter(bias_hh_l0[:costom_gru.gru.hb_r.size(0)])  # 更新门的偏置\n",
    "costom_gru.gru.hb_z = nn.Parameter(bias_hh_l0[costom_gru.gru.hb_r.size(0):costom_gru.gru.hb_z.size(0) + costom_gru.gru.hb_r.size(0)])  # 重置门的偏置\n",
    "costom_gru.gru.hb_h = nn.Parameter(bias_hh_l0[costom_gru.gru.hb_z.size(0) + costom_gru.gru.hb_r.size(0):])  # 候选隐藏状态的偏置\n",
    "\n",
    "costom_gru.gru.xb_r = nn.Parameter(bias_ih_l0[:costom_gru.gru.xb_r.size(0)])\n",
    "costom_gru.gru.xb_z = nn.Parameter(bias_ih_l0[costom_gru.gru.xb_r.size(0):costom_gru.gru.xb_z.size(0) + costom_gru.gru.xb_r.size(0)])\n",
    "costom_gru.gru.xb_h = nn.Parameter(bias_ih_l0[costom_gru.gru.xb_z.size(0) + costom_gru.gru.xb_r.size(0):])\n",
    "\n",
    "# 初始化输入数据\n",
    "x = torch.rand(seq_length, batch_size, input_size).to(device)\n",
    "\n",
    "# 获取CostomGRU和nn.GRU的输出\n",
    "output1, h1 = costom_gru(x)\n",
    "output2, h2 = gru(x)\n",
    "\n",
    "\n",
    "# 使用torch.allclose比较输出是否相等\n",
    "print(\"output1 == output2 ? {}\".format(torch.allclose(output1.to('cpu'), output2.to('cpu'), atol=1e-6)))\n",
    "print(\"h1 == h2 ? {}\".format(torch.allclose(h1.to('cpu'), h2.to('cpu'), atol=1e-6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3acd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5676e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
